# Triton Multimodal

Triton Inference Serverë¥¼ í™œìš©í•œ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ë©€í‹°ëª¨ë‹¬ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” LLM(Large Language Model) ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- **ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬**: ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì²˜ë¦¬
- **Triton Inference Server**: ê³ ì„±ëŠ¥ ëª¨ë¸ ì„œë¹™ì„ ìœ„í•œ NVIDIA Triton í™œìš©
- **FastAPI**: RESTful APIë¥¼ í†µí•œ ê°„í¸í•œ ì ‘ê·¼
- **ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›**: 
  - Gemma-3n ë©€í‹°ëª¨ë‹¬ ëª¨ë¸
  - vLLM ë°±ì—”ë“œ ì§€ì›
- **ì„ë² ë”© ì„œë¹„ìŠ¤**: í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ê¸°ëŠ¥

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
triton-multimodal/
â”œâ”€â”€ README.md                 # í”„ë¡œì íŠ¸ ì„¤ëª…ì„œ
â”œâ”€â”€ fastapi/                 # FastAPI ì›¹ ì„œë²„
â”‚   â””â”€â”€ app.py              # API ì„œë²„ ë©”ì¸ íŒŒì¼
â”œâ”€â”€ model_repository/        # Triton ëª¨ë¸ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ llm/                # LLM ëª¨ë¸ ì„¤ì •
â”‚   â””â”€â”€ embedding/          # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
```


## ğŸ’» ì‚¬ìš©ë²•

### 1. Triton ì„œë²„ ì‹œì‘

```bash
# 1. Git clone
git clone https://github.com/suranchoi/triton-multimodal-api
cd triton-multimodal-api

# 2. ë„ì»¤ ì´ë¯¸ì§€ ë¹Œë“œ (Dockerfile ì´ ìˆëŠ” ê²½ë¡œì—ì„œ)
docker build . -t triton-multimodal:latest

# 3. Triton ì„œë²„ ì‹¤í–‰
docker run -d -it --name triton-multimodal-api \
--gpus all \
--shm-size=1G \
--ulimit memlock=-1 --ulimit stack=67108864 \
-p 8000:8000 -p 8001:8001 -p 8002:8002 \
-v $(pwd)/model_repository:/models \
triton-multimodal:latest \
tritonserver --model-repository=/models
```

### 2. FastAPI ì„œë²„ ì‹œì‘

```bash
cd fastapi
python app.py
```

ë˜ëŠ”

```bash
uvicorn fastapi.app:app --host 0.0.0.0 --port 8080
```


## ğŸŒ API ì—”ë“œí¬ì¸íŠ¸

FastAPI ì„œë²„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

### ë©€í‹°ëª¨ë‹¬ ìƒì„± API

```http
POST /multimodal/generate
```

**ìš”ì²­:**
```bash
# 1. ì´ë¯¸ì§€ 
curl -s http://localhost:8080/multimodal/generate \
  -H "Content-Type: application/json" \
  -d '{
    "input_text": "ì´ë¯¸ì§€ë¥¼ ë¬˜ì‚¬í•´ì¤˜.",
    "image_data": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg"
  }'

# 2. ë¹„ë””ì˜¤
curl -s http://localhost:8080/multimodal/generate \
  -H "Content-Type: application/json" \
  -d '{
    "input_text": "ì´ ì˜ìƒì˜ ì¥ë©´ ì „í™˜ì„ ì„¤ëª…í•´ì¤˜.",
    "video_data": "/data2/llm/triton-multimodal-api/data/dks_llm.mp4"
  }'

# 3. ì˜¤ë””ì˜¤
curl -s http://localhost:8080/multimodal/generate \
  -H "Content-Type: application/json" \
  -d '{
    "input_text": "Transcribe this audio clip:",
    "audio_data": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct/resolve/main/examples/what_is_shown_in_this_image.wav"
  }'
```

### ì„ë² ë”© ìƒì„± API

```http
POST /embeddings/{model_name}
```

**ìš”ì²­:**
```bash
curl -X POST "http://localhost:8080/embeddings/sentence-transformers/all-MiniLM-L6-v2" \
  -H "Content-Type: application/json" \
  -d '{"input": "ì•ˆë…•í•˜ì„¸ìš”"}'
```


## ğŸ” ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```bash
   # ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì • ì¡°ì •
   tensor_parallel_size=2
   max_model_len=2048
   gpu_memory_utilization=0.8
   ```

2. **FFmpeg ì„¤ì¹˜ í•„ìš”**
   ```bash
   # Ubuntu/Debian
   sudo apt-get install ffmpeg
   
   # macOS
   brew install ffmpeg
   ```

3. **ëª¨ë¸ ê²½ë¡œ ì˜¤ë¥˜**
   - ëª¨ë¸ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
   - ëª¨ë¸ íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸


### ë¡œê·¸ í™•ì¸

```bash
# Triton ì„œë²„ ë¡œê·¸ í™•ì¸
docker logs -f <triton_container_id>
```